"""
Firebase Functions - Smart OCR RAG
S·ª≠ d·ª•ng Google Vision OCR + OpenAI + Semantic Search ƒë·ªÉ tr√≠ch xu·∫•t th√†nh ph·∫ßn s·∫£n ph·∫©m
"""
import os
import json
import base64
import logging
import uuid
from io import BytesIO
from datetime import datetime

from firebase_functions import https_fn, options
from firebase_admin import initialize_app, db, storage

# --- KH·ªûI T·∫†O FIREBASE ---
# C·∫•u h√¨nh cho Realtime Database v√† Storage
firebase_app = initialize_app(options={
    'databaseURL': 'https://hackathon-2026-482104-default-rtdb.firebaseio.com/',
    'storageBucket': 'hackathon-2026-482104.firebasestorage.app'
})

# --- LAZY LOADING CHO C√ÅC TH∆Ø VI·ªÜN N·∫∂NG ---
# S·ª≠ d·ª•ng lazy loading ƒë·ªÉ t·ªëi ∆∞u cold start
_vision_client = None
_openai_client = None

def get_openai_embeddings(texts: list[str]) -> list[list[float]]:
    """
    Get embeddings from OpenAI API instead of local model
    Args:
        texts: List of text strings to embed
    Returns:
        List of embedding vectors
    """
    client = get_openai_client()
    
    try:
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        return [item.embedding for item in response.data]
    except Exception as e:
        logging.error(f"Error getting OpenAI embeddings: {e}")
        raise

def get_vision_client():
    """Lazy load Google Vision client"""
    global _vision_client
    if _vision_client is None:
        from google.cloud import vision
        _vision_client = vision.ImageAnnotatorClient()
    return _vision_client

def get_openai_client():
    """Lazy load OpenAI client"""
    global _openai_client
    if _openai_client is None:
        from openai import OpenAI
        # L·∫•y API key t·ª´ environment variable
        api_key = os.environ.get('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh!")
        _openai_client = OpenAI(api_key=api_key)
    return _openai_client


# ---------------------------------------------------------
# B∆Ø·ªöC 1: GOOGLE VISION OCR (L·∫•y d·ªØ li·ªáu th√¥)
# ---------------------------------------------------------
def get_ocr_data(image_content: bytes) -> list:
    """
    S·ª≠ d·ª•ng Google Vision ƒë·ªÉ OCR ·∫£nh
    Args:
        image_content: bytes c·ªßa ·∫£nh
    Returns:
        List c√°c t·ª´ v·ªõi v·ªã tr√≠ bounding box
    """
    from google.cloud import vision
    
    client = get_vision_client()
    image = vision.Image(content=image_content)
    
    response = client.document_text_detection(image=image)
    
    if not response.text_annotations:
        return []

    word_list = []
    ignore_chars = [",", ".", ":", ";", "|", "(", ")", "[", "]", "{", "}", "-", "*", "%"]
    
    for page in response.full_text_annotation.pages:
        for block in page.blocks:
            for paragraph in block.paragraphs:
                for word in paragraph.words:
                    word_text = ''.join([symbol.text for symbol in word.symbols])
                    box = [(v.x, v.y) for v in word.bounding_box.vertices]
                    is_noise = word_text in ignore_chars
                    
                    word_list.append({
                        "text": word_text, 
                        "box": box,
                        "is_noise": is_noise
                    })
    
    return word_list


# ---------------------------------------------------------
# B∆Ø·ªöC 2: OPENAI ANALYSIS (Strict Prompt)
# ---------------------------------------------------------
def analyze_with_openai_strict(ocr_word_list: list) -> list:
    """
    S·ª≠ d·ª•ng OpenAI ƒë·ªÉ ph√¢n t√≠ch v√† tr√≠ch xu·∫•t nguy√™n li·ªáu
    """
    full_text = " ".join([w['text'] for w in ocr_word_list])
    
    client = get_openai_client()
    
    prompt = f"""
    B·∫°n l√† m·ªôt h·ªá th·ªëng tr√≠ch xu·∫•t d·ªØ li·ªáu OCR ch√≠nh x√°c (OCR Post-processor).
    
    INPUT: M·ªôt ƒëo·∫°n vƒÉn b·∫£n th√¥ t·ª´ bao b√¨ s·∫£n ph·∫©m.
    TASK: Tr√≠ch xu·∫•t danh s√°ch c√°c "Th√†nh ph·∫ßn nguy√™n li·ªáu" (Ingredients).
    
    Y√äU C·∫¶U C·ª∞C K·ª≤ QUAN TR·ªåNG (STRICT RULES):
    1. T√°ch ri√™ng t·ª´ng nguy√™n li·ªáu. D·∫•u ph·∫©y (,) l√† d·∫•u hi·ªáu ng·∫Øt quan tr·ªçng nh·∫•t.
    2. LO·∫†I B·ªé ho√†n to√†n c√°c con s·ªë ph·∫ßn trƒÉm v√† ƒë·ªãnh l∆∞·ª£ng (V√≠ d·ª•: "B∆° (1,9%)" -> Ch·ªâ l·∫•y "B∆°").
    3. LO·∫†I B·ªé c√°c m√£ ph·ª• gia trong ngo·∫∑c n·∫øu c√≥ th·ªÉ t√°ch r·ªùi.
    4. GI·ªÆ NGUY√äN ch√≠nh t·∫£ c·ªßa vƒÉn b·∫£n g·ªëc (k·ªÉ c·∫£ l·ªói sai).
    5. Output tr·∫£ v·ªÅ JSON format: {{ "ingredients": ["item1", "item2", ...] }}
    
    VƒÉn b·∫£n Input:
    '''
    {full_text}
    '''
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0
        )
        data = json.loads(response.choices[0].message.content)
        return data.get("ingredients", [])
    except Exception as e:
        logging.error(f"L·ªói OpenAI: {e}")
        return []


# ---------------------------------------------------------
# B∆Ø·ªöC 2.5: PH√ÇN T√çCH R·ª¶I RO S·ª®C KH·ªéE (Health Risk Analysis)
# ---------------------------------------------------------
def analyze_health_risks(ingredients: list, health_profile: dict) -> dict:
    """
    S·ª≠ d·ª•ng OpenAI ƒë·ªÉ ph√¢n t√≠ch r·ªßi ro s·ª©c kh·ªèe d·ª±a tr√™n ingredients v√† health profile
    
    Args:
        ingredients: Danh s√°ch nguy√™n li·ªáu ƒë√£ tr√≠ch xu·∫•t
        health_profile: H·ªì s∆° s·ª©c kh·ªèe c·ªßa ng∆∞·ªùi d√πng
            {
                "medical_history": ["b·ªánh 1", "b·ªánh 2"],
                "allergy": ["d·ªã ·ª©ng 1", "d·ªã ·ª©ng 2"]
            }
    
    Returns:
        Dictionary ch·ª©a warnings, safe_ingredients, overall_recommendation
    """
    client = get_openai_client()
    
    # Format health profile for prompt
    medical_history = health_profile.get('medical_history', [])
    allergies = health_profile.get('allergy', [])
    
    medical_history_str = ", ".join(medical_history) if medical_history else "Kh√¥ng c√≥"
    allergies_str = ", ".join(allergies) if allergies else "Kh√¥ng c√≥"
    ingredients_str = ", ".join(ingredients)
    
    prompt = f"""
B·∫°n l√† m·ªôt B√ÅC Sƒ® DINH D∆Ø·ª†NG v√† CHUY√äN GIA D·ªä ·ª®NG TH·ª∞C PH·∫®M v·ªõi ki·∫øn th·ª©c y khoa s√¢u r·ªông.

## NHI·ªÜM V·ª§
Ph√¢n t√≠ch danh s√°ch TH√ÄNH PH·∫¶N th·ª±c ph·∫©m v√† x√°c ƒë·ªãnh th√†nh ph·∫ßn n√†o c√≥ th·ªÉ G√ÇY H·∫†I cho ng∆∞·ªùi d√πng d·ª±a tr√™n H·ªí S∆† S·ª®C KH·ªéE c·ªßa h·ªç.

## H·ªí S∆† S·ª®C KH·ªéE
- Ti·ªÅn s·ª≠ b·ªánh l√Ω: {medical_history_str}
- D·ªã ·ª©ng ƒë√£ bi·∫øt: {allergies_str}

## DANH S√ÅCH TH√ÄNH PH·∫¶N C·∫¶N PH√ÇN T√çCH
{ingredients_str}

## Y√äU C·∫¶U PH√ÇN T√çCH (QUAN TR·ªåNG)

1. **Nh·∫≠n di·ªán tr·ª±c ti·∫øp**: Th√†nh ph·∫ßn C√ì TRONG danh s√°ch d·ªã ·ª©ng
   - V√≠ d·ª•: "h·∫£i s·∫£n" bao g·ªìm: t√¥m, cua, m·ª±c, s√≤, ·ªëc, c√°...
   - V√≠ d·ª•: "c√°c lo·∫°i ƒë·∫≠u" bao g·ªìm: ƒë·∫≠u ph·ªông, ƒë·∫≠u n√†nh, ƒë·∫≠u xanh, ƒë·∫≠u ƒë·ªè...
   - V√≠ d·ª•: "gluten" bao g·ªìm: b·ªôt m√¨, l√∫a m·∫°ch, y·∫øn m·∫°ch...

2. **Nh·∫≠n di·ªán gi√°n ti·∫øp (Cross-reactivity)**: Th√†nh ph·∫ßn c√≥ th·ªÉ G√ÇY PH·∫¢N ·ª®NG CH√âO
   - V√≠ d·ª•: D·ªã ·ª©ng latex ‚Üí c√≥ th·ªÉ ph·∫£n ·ª©ng v·ªõi chu·ªëi, b∆°, kiwi
   - V√≠ d·ª•: D·ªã ·ª©ng ƒë·∫≠u ph·ªông ‚Üí c√≥ th·ªÉ ph·∫£n ·ª©ng v·ªõi ƒë·∫≠u t∆∞∆°ng, ƒë·∫≠u xanh
   - V√≠ d·ª•: D·ªã ·ª©ng s·ªØa b√≤ ‚Üí c√≥ th·ªÉ ph·∫£n ·ª©ng v·ªõi s·ªØa d√™, s·ªØa c·ª´u

3. **·∫¢nh h∆∞·ªüng ti·ªÅn s·ª≠ b·ªánh**: Th√†nh ph·∫ßn KH√îNG T·ªêT cho t√¨nh tr·∫°ng b·ªánh l√Ω
   - Gan nhi·ªÖm m·ª° ‚Üí h·∫°n ch·∫ø ƒë∆∞·ªùng, ch·∫•t b√©o b√£o h√≤a, r∆∞·ª£u, fructose
   - Ti·ªÉu ƒë∆∞·ªùng ‚Üí h·∫°n ch·∫ø ƒë∆∞·ªùng, tinh b·ªôt tinh ch·∫ø, carbohydrate ƒë∆°n gi·∫£n
   - Cao huy·∫øt √°p ‚Üí h·∫°n ch·∫ø mu·ªëi (sodium), MSG, th·ª±c ph·∫©m ch·∫ø bi·∫øn s·∫µn
   - Vi√™m h·ªçng ‚Üí h·∫°n ch·∫ø ƒë·ªì cay, ƒë·ªì l·∫°nh, ƒë·ªì chi√™n r√°n, th·ª±c ph·∫©m c√≥ t√≠nh axit
   - Gout ‚Üí h·∫°n ch·∫ø purine (th·ªãt ƒë·ªè, n·ªôi t·∫°ng, h·∫£i s·∫£n)
   - B·ªánh th·∫≠n ‚Üí h·∫°n ch·∫ø protein, potassium, phosphorus

## OUTPUT FORMAT (JSON)
{{
  "warnings": [
    {{
      "ingredient": "T√™n th√†nh ph·∫ßn g·ªëc t·ª´ danh s√°ch",
      "risk_score": 0.95,
      "warning_type": "allergy/cross_reactivity/medical_condition",
      "summary": "T√≥m t·∫Øt ng·∫Øn g·ªçn l√Ω do c·∫£nh b√°o",
      "scientific_explanation": "Gi·∫£i th√≠ch CHI TI·∫æT v·ªÅ m·∫∑t y khoa/sinh h·ªçc: t√™n khoa h·ªçc c·ªßa th√†nh ph·∫ßn, c∆° ch·∫ø sinh h·ªçc t·∫°i sao g√¢y h·∫°i, c√°c protein/h·ª£p ch·∫•t c·ª• th·ªÉ li√™n quan, qu√° tr√¨nh ph·∫£n ·ª©ng trong c∆° th·ªÉ",
      "potential_effects": ["T√°c ƒë·ªông 1", "T√°c ƒë·ªông 2", "T√°c ƒë·ªông 3"],
      "recommendation": "L·ªùi khuy√™n c·ª• th·ªÉ v√† th·ª±c t·∫ø cho b·ªánh nh√¢n"
    }}
  ],
  "safe_ingredients": ["Danh s√°ch c√°c th√†nh ph·∫ßn AN TO√ÄN kh√¥ng c√≥ v·∫•n ƒë·ªÅ"],
  "overall_recommendation": "ƒê√°nh gi√° t·ªïng th·ªÉ: s·∫£n ph·∫©m n√†y c√≥ AN TO√ÄN hay KH√îNG AN TO√ÄN cho b·ªánh nh√¢n, k√®m l·ªùi khuy√™n cu·ªëi c√πng"
}}

## QUY T·∫ÆC B·∫ÆT BU·ªòC
- Ch·ªâ tr·∫£ v·ªÅ JSON thu·∫ßn t√∫y, kh√¥ng c√≥ text gi·∫£i th√≠ch b√™n ngo√†i
- TO√ÄN B·ªò n·ªôi dung PH·∫¢I vi·∫øt b·∫±ng TI·∫æNG VI·ªÜT C√ì D·∫§U ƒë·∫ßy ƒë·ªß
- risk_score: ƒêi·ªÉm s·ªë ƒë√°nh gi√° m·ª©c ƒë·ªô nguy hi·ªÉm trong kho·∫£ng [0, 1], trong ƒë√≥:
  * 0.8 - 1.0 = C·ª±c k·ª≥ nguy hi·ªÉm (d·ªã ·ª©ng tr·ª±c ti·∫øp, c√≥ th·ªÉ g√¢y s·ªëc ph·∫£n v·ªá)
  * 0.6 - 0.79 = Nguy hi·ªÉm cao (ph·∫£n ·ª©ng ch√©o m·∫°nh, ·∫£nh h∆∞·ªüng nghi√™m tr·ªçng ƒë·∫øn b·ªánh l√Ω)
  * 0.4 - 0.59 = Nguy hi·ªÉm trung b√¨nh (·∫£nh h∆∞·ªüng ti·ªÅn s·ª≠ b·ªánh, c·∫ßn h·∫°n ch·∫ø)
  * 0.2 - 0.39 = Nguy hi·ªÉm th·∫•p (c·∫ßn th·∫≠n tr·ªçng, theo d√µi)
  * 0.0 - 0.19 = R·∫•t th·∫•p (·∫£nh h∆∞·ªüng nh·∫π, c√≥ th·ªÉ s·ª≠ d·ª•ng v·ªõi l∆∞·ª£ng nh·ªè)
- Gi·∫£i th√≠ch khoa h·ªçc ph·∫£i chuy√™n s√¢u nh∆∞ng v·∫´n d·ªÖ hi·ªÉu cho ng∆∞·ªùi kh√¥ng c√≥ chuy√™n m√¥n y khoa
- N·∫øu KH√îNG c√≥ th√†nh ph·∫ßn n√†o c√≥ v·∫•n ƒë·ªÅ, tr·∫£ v·ªÅ warnings = [] v√† overall_recommendation t√≠ch c·ª±c
- Ch·ªâ c·∫£nh b√°o nh·ªØng th√†nh ph·∫ßn TH·ª∞C S·ª∞ c√≥ trong danh s√°ch, kh√¥ng t·ª± th√™m th√†nh ph·∫ßn m·ªõi
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0
        )
        data = json.loads(response.choices[0].message.content)
        return {
            "warnings": data.get("warnings", []),
            "safe_ingredients": data.get("safe_ingredients", []),
            "overall_recommendation": data.get("overall_recommendation", "")
        }
    except Exception as e:
        logging.error(f"L·ªói ph√¢n t√≠ch health risks: {e}")
        return {
            "warnings": [],
            "safe_ingredients": ingredients,
            "overall_recommendation": f"Kh√¥ng th·ªÉ ph√¢n t√≠ch r·ªßi ro s·ª©c kh·ªèe: {str(e)}"
        }


# ---------------------------------------------------------
# B∆Ø·ªöC 3: SEMANTIC MAPPING RAG (Core Logic)
# ---------------------------------------------------------
def find_coordinates_semantic(target_phrases: list, ocr_word_list: list, threshold: float = 0.55) -> list:
    """
    S·ª≠ d·ª•ng OpenAI Embeddings API ƒë·ªÉ t√¨m v·ªã tr√≠ c·ªßa t·ª´ng nguy√™n li·ªáu trong ·∫£nh
    """
    import numpy as np
    from numpy.linalg import norm
    
    results = []
    
    # T·∫°o Corpus t·ª´ OCR data
    corpus_texts = []
    corpus_indices = []
    
    clean_indices = [i for i, w in enumerate(ocr_word_list) if not w['is_noise']]
    max_window_size = 3  # Reduced from 5 for better performance
    
    for window in range(1, max_window_size + 1):
        for i in range(len(clean_indices) - window + 1):
            current_indices = clean_indices[i : i + window]
            text_segment = " ".join([ocr_word_list[idx]['text'] for idx in current_indices])
            corpus_texts.append(text_segment)
            corpus_indices.append(current_indices)
    
    if not corpus_texts:
        return []
    
    # Batch encode corpus v√† queries v·ªõi OpenAI
    all_texts = corpus_texts + target_phrases
    all_embeddings = get_openai_embeddings(all_texts)
    
    corpus_embeddings = np.array(all_embeddings[:len(corpus_texts)])
    query_embeddings = np.array(all_embeddings[len(corpus_texts):])
    
    # Batch cosine similarity calculation
    # Normalize embeddings
    corpus_norms = norm(corpus_embeddings, axis=1, keepdims=True)
    query_norms = norm(query_embeddings, axis=1, keepdims=True)
    
    normalized_corpus = corpus_embeddings / (corpus_norms + 1e-8)
    normalized_queries = query_embeddings / (query_norms + 1e-8)
    
    # Compute all similarities at once
    all_similarities = np.dot(normalized_queries, normalized_corpus.T)
    
    # Process each query
    for i, phrase in enumerate(target_phrases):
        similarities = all_similarities[i]
        best_idx = np.argmax(similarities)
        best_score = float(similarities[best_idx])
        
        if best_score >= threshold:
            matched_text = corpus_texts[best_idx]
            matched_raw_indices = corpus_indices[best_idx]
            
            # L·∫•y bounding box
            matched_boxes = [ocr_word_list[idx]['box'] for idx in matched_raw_indices]
            
            all_x = [pt[0] for box in matched_boxes for pt in box]
            all_y = [pt[1] for box in matched_boxes for pt in box]
            
            final_box = [
                [min(all_x), min(all_y)], 
                [max(all_x), min(all_y)], 
                [max(all_x), max(all_y)], 
                [min(all_x), max(all_y)]
            ]
            
            results.append({
                "label": phrase,
                "matched_text": matched_text,
                "confidence": round(best_score, 3),
                "bounding_box": final_box
            })
    
    return results


# ---------------------------------------------------------
# FIREBASE FUNCTION ENDPOINT
# ---------------------------------------------------------
@https_fn.on_request(
    cors=options.CorsOptions(
        cors_origins=["*"],  # Cho ph√©p t·∫•t c·∫£ origins, c√≥ th·ªÉ restrict l·∫°i
        cors_methods=["GET", "POST"]
    ),
    memory=options.MemoryOption.GB_1,  # Reduced from GB_2 since no local model
    timeout_sec=300,  # 5 ph√∫t timeout
    region="asia-southeast1",  # Region Singapore
    min_instances=1  # Keep 1 instance warm to eliminate cold start
)
def smart_ocr_rag(req: https_fn.Request) -> https_fn.Response:
    """
    Firebase HTTP Function ƒë·ªÉ x·ª≠ l√Ω OCR + RAG + Health Analysis
    
    Request Body (JSON):
    {
        "image_base64": "base64_encoded_image_string",
        "threshold": 0.6  (optional, default 0.6),
        "health_profile": {
            "medical_history": ["b·ªánh 1", "b·ªánh 2"],
            "allergy": ["d·ªã ·ª©ng 1", "d·ªã ·ª©ng 2"]
        }
    }
    
    Ho·∫∑c g·ª≠i qua multipart/form-data:
    - image: file ·∫£nh
    - health_profile: JSON string c·ªßa health profile
    - threshold: optional
    """
    
    # Ch·ªâ ch·∫•p nh·∫≠n POST
    if req.method != 'POST':
        return https_fn.Response(
            json.dumps({"error": "Method not allowed. Use POST."}),
            status=405,
            headers={"Content-Type": "application/json"}
        )
    
    try:
        image_content = None
        threshold = 0.6
        health_profile = None
        
        # X·ª≠ l√Ω multipart/form-data (upload file tr·ª±c ti·∫øp)
        if req.files and 'image' in req.files:
            file = req.files['image']
            image_content = file.read()
            threshold = float(req.form.get('threshold', 0.6))
            
            # Parse health_profile t·ª´ form data
            health_profile_str = req.form.get('health_profile')
            if health_profile_str:
                try:
                    health_profile = json.loads(health_profile_str)
                except json.JSONDecodeError:
                    return https_fn.Response(
                        json.dumps({"error": "Invalid health_profile JSON format"}),
                        status=400,
                        headers={"Content-Type": "application/json"}
                    )
        
        # X·ª≠ l√Ω JSON body (base64 image)
        elif req.is_json:
            data = req.get_json()
            
            if 'image_base64' not in data:
                return https_fn.Response(
                    json.dumps({"error": "Missing 'image_base64' field"}),
                    status=400,
                    headers={"Content-Type": "application/json"}
                )
            
            # Decode base64
            image_base64 = data['image_base64']
            # X√≥a prefix n·∫øu c√≥ (data:image/png;base64,...)
            if ',' in image_base64:
                image_base64 = image_base64.split(',')[1]
            
            image_content = base64.b64decode(image_base64)
            threshold = float(data.get('threshold', 0.6))
            health_profile = data.get('health_profile')
        
        else:
            return https_fn.Response(
                json.dumps({"error": "Invalid request format. Use JSON or multipart/form-data"}),
                status=400,
                headers={"Content-Type": "application/json"}
            )
        
        # Validate health_profile (b·∫Øt bu·ªôc)
        if not health_profile:
            return https_fn.Response(
                json.dumps({
                    "error": "Missing 'health_profile' field",
                    "required_format": {
                        "medical_history": ["b·ªánh 1", "b·ªánh 2"],
                        "allergy": ["d·ªã ·ª©ng 1", "d·ªã ·ª©ng 2"]
                    }
                }),
                status=400,
                headers={"Content-Type": "application/json"}
            )
        
        # Validate health_profile structure
        if not isinstance(health_profile.get('medical_history'), list):
            health_profile['medical_history'] = []
        if not isinstance(health_profile.get('allergy'), list):
            health_profile['allergy'] = []
        
        # ===== X·ª¨ L√ù CH√çNH =====
        
        # 1. OCR
        logging.info("üîç B·∫Øt ƒë·∫ßu OCR...")
        ocr_data = get_ocr_data(image_content)
        
        if not ocr_data:
            return https_fn.Response(
                json.dumps({
                    "success": False,
                    "error": "Kh√¥ng t√¨m th·∫•y text trong ·∫£nh"
                }),
                status=200,
                headers={"Content-Type": "application/json"}
            )
        
        # 2. Ph√¢n t√≠ch v·ªõi OpenAI ƒë·ªÉ tr√≠ch xu·∫•t nguy√™n li·ªáu
        logging.info("ü§ñ ƒêang ph√¢n t√≠ch v·ªõi AI...")
        ingredients = analyze_with_openai_strict(ocr_data)
        
        if not ingredients:
            # Tr·∫£ v·ªÅ raw OCR n·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c
            raw_text = " ".join([w['text'] for w in ocr_data if not w['is_noise']])
            return https_fn.Response(
                json.dumps({
                    "success": True,
                    "ingredients": [],
                    "health_warnings": [],
                    "safe_ingredients": [],
                    "risk_summary": {
                        "max_risk_score": 0,
                        "avg_risk_score": 0,
                        "critical_risk_count": 0,
                        "high_risk_count": 0,
                        "medium_risk_count": 0,
                        "low_risk_count": 0,
                        "very_low_risk_count": 0,
                        "total_warnings": 0,
                        "overall_recommendation": "Kh√¥ng t√¨m th·∫•y nguy√™n li·ªáu ƒë·ªÉ ph√¢n t√≠ch."
                    },
                    "mappings": [],
                    "raw_text": raw_text,
                    "message": "Kh√¥ng t√¨m th·∫•y nguy√™n li·ªáu. Tr·∫£ v·ªÅ raw OCR text.",
                    "user_profile": health_profile
                }, ensure_ascii=False),
                status=200,
                headers={"Content-Type": "application/json; charset=utf-8"}
            )
        
        # 3. Ph√¢n t√≠ch r·ªßi ro s·ª©c kh·ªèe
        logging.info("üè• ƒêang ph√¢n t√≠ch r·ªßi ro s·ª©c kh·ªèe...")
        health_analysis = analyze_health_risks(ingredients, health_profile)
        
        # 4. Semantic Mapping
        logging.info("üîó ƒêang mapping v·ªã tr√≠...")
        mappings = find_coordinates_semantic(ingredients, ocr_data, threshold)
        
        # 5. T√≠nh to√°n risk summary d·ª±a tr√™n risk_score
        warnings = health_analysis.get("warnings", [])
        
        # Ph√¢n lo·∫°i theo risk_score
        critical_risk_count = len([w for w in warnings if w.get("risk_score", 0) >= 0.8])  # 0.8-1.0
        high_risk_count = len([w for w in warnings if 0.6 <= w.get("risk_score", 0) < 0.8])  # 0.6-0.79
        medium_risk_count = len([w for w in warnings if 0.4 <= w.get("risk_score", 0) < 0.6])  # 0.4-0.59
        low_risk_count = len([w for w in warnings if 0.2 <= w.get("risk_score", 0) < 0.4])  # 0.2-0.39
        very_low_risk_count = len([w for w in warnings if w.get("risk_score", 0) < 0.2])  # 0-0.19
        
        # T√≠nh max v√† avg risk score
        risk_scores = [w.get("risk_score", 0) for w in warnings]
        max_risk_score = max(risk_scores) if risk_scores else 0
        avg_risk_score = sum(risk_scores) / len(risk_scores) if risk_scores else 0
        
        # 6. T·∫°o response
        response_data = {
            "success": True,
            "ingredients": ingredients,
            "health_warnings": warnings,
            "safe_ingredients": health_analysis.get("safe_ingredients", []),
            "risk_summary": {
                "max_risk_score": round(max_risk_score, 2),
                "avg_risk_score": round(avg_risk_score, 2),
                "critical_risk_count": critical_risk_count,
                "high_risk_count": high_risk_count,
                "medium_risk_count": medium_risk_count,
                "low_risk_count": low_risk_count,
                "very_low_risk_count": very_low_risk_count,
                "total_warnings": len(warnings),
                "overall_recommendation": health_analysis.get("overall_recommendation", "")
            },
            "mappings": mappings,
            "total_ocr_words": len(ocr_data),
            "matched_count": len(mappings),
            "threshold_used": threshold,
            "user_profile": {
                "allergies_checked": health_profile.get("allergy", []),
                "conditions_checked": health_profile.get("medical_history", [])
            }
        }
        
        logging.info(f"‚úÖ Ho√†n th√†nh! T√¨m th·∫•y {len(ingredients)} nguy√™n li·ªáu, {len(warnings)} c·∫£nh b√°o")
        
        return https_fn.Response(
            json.dumps(response_data, ensure_ascii=False),
            status=200,
            headers={"Content-Type": "application/json; charset=utf-8"}
        )
        
    except Exception as e:
        logging.error(f"‚ùå Error: {str(e)}")
        return https_fn.Response(
            json.dumps({"success": False, "error": str(e)}),
            status=500,
            headers={"Content-Type": "application/json"}
        )


# ---------------------------------------------------------
# HEALTH CHECK ENDPOINT
# ---------------------------------------------------------
@https_fn.on_request(
    cors=options.CorsOptions(cors_origins=["*"], cors_methods=["GET"]),
    region="asia-southeast1"
)
def health_check(req: https_fn.Request) -> https_fn.Response:
    """Simple health check endpoint"""
    return https_fn.Response(
        json.dumps({
            "status": "healthy",
            "service": "smart-ocr-rag",
            "version": "1.0.0"
        }),
        status=200,
        headers={"Content-Type": "application/json"}
    )


# ---------------------------------------------------------
# SAVE HISTORY ENDPOINT
# ---------------------------------------------------------
@https_fn.on_request(
    cors=options.CorsOptions(
        cors_origins=["*"],
        cors_methods=["POST"]
    ),
    memory=options.MemoryOption.MB_512,
    timeout_sec=60,
    region="asia-southeast1"
)
def save_history(req: https_fn.Request) -> https_fn.Response:
    """
    L∆∞u l·ªãch s·ª≠ scan v√†o Realtime Database + Upload ·∫£nh l√™n Storage
    
    H·ªó tr·ª£ 2 c√°ch g·ª≠i request:
    
    1. JSON Body:
    {
        "device_id": "unique_device_identifier",
        "image_base64": "base64_encoded_image",
        "scan_result": {...}
    }
    
    2. Multipart Form-Data:
    - device_id: string
    - image: file (·∫£nh)
    - scan_result: JSON string
    """
    
    if req.method != 'POST':
        return https_fn.Response(
            json.dumps({"error": "Method not allowed. Use POST."}),
            status=405,
            headers={"Content-Type": "application/json"}
        )
    
    try:
        device_id = None
        image_content = None
        scan_result = None
        
        # === C√ÅCH 1: Multipart Form-Data (upload file tr·ª±c ti·∫øp) ===
        if req.files and 'image' in req.files:
            file = req.files['image']
            image_content = file.read()
            
            device_id = req.form.get('device_id')
            
            # Parse scan_result t·ª´ form data (JSON string)
            scan_result_str = req.form.get('scan_result')
            if scan_result_str:
                try:
                    scan_result = json.loads(scan_result_str)
                except json.JSONDecodeError:
                    return https_fn.Response(
                        json.dumps({"error": "Invalid scan_result JSON format"}),
                        status=400,
                        headers={"Content-Type": "application/json"}
                    )
        
        # === C√ÅCH 2: JSON Body (base64 image) ===
        elif req.is_json:
            data = req.get_json()
            
            device_id = data.get('device_id')
            scan_result = data.get('scan_result')
            
            # Decode base64 image n·∫øu c√≥
            image_base64 = data.get('image_base64')
            if image_base64:
                # X√≥a prefix n·∫øu c√≥ (data:image/png;base64,...)
                if ',' in image_base64:
                    image_base64 = image_base64.split(',')[1]
                image_content = base64.b64decode(image_base64)
        
        else:
            return https_fn.Response(
                json.dumps({"error": "Invalid request format. Use JSON or multipart/form-data"}),
                status=400,
                headers={"Content-Type": "application/json"}
            )
        
        # Validate required fields
        if not device_id:
            return https_fn.Response(
                json.dumps({"error": "Missing 'device_id' field"}),
                status=400,
                headers={"Content-Type": "application/json"}
            )
        
        if not scan_result:
            return https_fn.Response(
                json.dumps({"error": "Missing 'scan_result' field"}),
                status=400,
                headers={"Content-Type": "application/json"}
            )
        
        # Generate unique filename v√† timestamp
        timestamp = int(datetime.now().timestamp() * 1000)
        unique_id = str(uuid.uuid4())[:8]
        
        image_url = None
        
        # Upload image to Storage n·∫øu c√≥
        if image_content:
            try:
                # Upload to Firebase Storage
                bucket = storage.bucket()
                blob_path = f"scan_images/{device_id}/{timestamp}_{unique_id}.jpg"
                blob = bucket.blob(blob_path)
                
                blob.upload_from_string(
                    image_content,
                    content_type='image/jpeg'
                )
                
                # Make the blob publicly accessible
                blob.make_public()
                image_url = blob.public_url
                
                logging.info(f"‚úÖ Uploaded image to: {image_url}")
                
            except Exception as e:
                logging.error(f"‚ùå Error uploading image: {e}")
                # Continue without image URL
                image_url = None
        
        # Prepare data for Realtime Database
        history_data = {
            "created_at": timestamp,
            "image_url": image_url,
            
            # Ingredients data
            "ingredients": scan_result.get("ingredients", []),
            "safe_ingredients": scan_result.get("safe_ingredients", []),
            
            # Health warnings (full object)
            "health_warnings": scan_result.get("health_warnings", []),
            
            # Risk summary (full object)
            "risk_summary": scan_result.get("risk_summary", {}),
            
            # Mappings (bounding boxes)
            "mappings": scan_result.get("mappings", []),
            
            # OCR metadata
            "total_ocr_words": scan_result.get("total_ocr_words", 0),
            "matched_count": scan_result.get("matched_count", 0),
            "threshold_used": scan_result.get("threshold_used", 0.6),
            
            # User profile
            "user_profile": scan_result.get("user_profile", {})
        }
        
        # Save to Realtime Database
        ref = db.reference(f'scan_history/{device_id}')
        new_ref = ref.push(history_data)
        history_id = new_ref.key
        
        logging.info(f"‚úÖ Saved history: {history_id} for device: {device_id}")
        
        return https_fn.Response(
            json.dumps({
                "success": True,
                "history_id": history_id,
                "image_url": image_url,
                "created_at": timestamp
            }, ensure_ascii=False),
            status=200,
            headers={"Content-Type": "application/json; charset=utf-8"}
        )
        
    except Exception as e:
        logging.error(f"‚ùå Error saving history: {str(e)}")
        return https_fn.Response(
            json.dumps({"success": False, "error": str(e)}),
            status=500,
            headers={"Content-Type": "application/json"}
        )


# ---------------------------------------------------------
# GET HISTORY ENDPOINT
# ---------------------------------------------------------
@https_fn.on_request(
    cors=options.CorsOptions(
        cors_origins=["*"],
        cors_methods=["GET"]
    ),
    memory=options.MemoryOption.MB_256,
    timeout_sec=30,
    region="asia-southeast1"
)
def get_history(req: https_fn.Request) -> https_fn.Response:
    """
    L·∫•y l·ªãch s·ª≠ scan t·ª´ Realtime Database
    
    Query Parameters:
    - device_id: (required) Device identifier
    - limit: (optional) Max items to return, default 20, max 100
    """
    
    if req.method != 'GET':
        return https_fn.Response(
            json.dumps({"error": "Method not allowed. Use GET."}),
            status=405,
            headers={"Content-Type": "application/json"}
        )
    
    try:
        # Get query parameters
        device_id = req.args.get('device_id')
        limit = min(int(req.args.get('limit', 20)), 100)  # Max 100 items
        
        if not device_id:
            return https_fn.Response(
                json.dumps({"error": "Missing 'device_id' query parameter"}),
                status=400,
                headers={"Content-Type": "application/json"}
            )
        
        # Query Realtime Database
        ref = db.reference(f'scan_history/{device_id}')
        
        # Get data ordered by created_at (descending - newest first)
        # Realtime DB orders ascending by default, so we get all and reverse
        snapshot = ref.order_by_child('created_at').limit_to_last(limit).get()
        
        if not snapshot:
            return https_fn.Response(
                json.dumps({
                    "success": True,
                    "history": [],
                    "count": 0
                }, ensure_ascii=False),
                status=200,
                headers={"Content-Type": "application/json; charset=utf-8"}
            )
        
        # Convert to list and add ID
        history_list = []
        for history_id, history_data in snapshot.items():
            history_item = {
                "id": history_id,
                **history_data
            }
            history_list.append(history_item)
        
        # Sort by created_at descending (newest first)
        history_list.sort(key=lambda x: x.get('created_at', 0), reverse=True)
        
        logging.info(f"‚úÖ Retrieved {len(history_list)} history items for device: {device_id}")
        
        return https_fn.Response(
            json.dumps({
                "success": True,
                "history": history_list,
                "count": len(history_list)
            }, ensure_ascii=False),
            status=200,
            headers={"Content-Type": "application/json; charset=utf-8"}
        )
        
    except Exception as e:
        logging.error(f"‚ùå Error getting history: {str(e)}")
        return https_fn.Response(
            json.dumps({"success": False, "error": str(e)}),
            status=500,
            headers={"Content-Type": "application/json"}
        )
